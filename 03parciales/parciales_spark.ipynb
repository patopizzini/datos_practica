{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd\n",
    "\n",
    "# create the Spark Session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# create the Spark Context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARCIAL\n",
    "\n",
    "Se tiene información estadística de la temporada regular de todos los jugadores de la NBA en un RDD de tuplas con el siguiente formato: (id_jugador, nombre, promedio_puntos, promedio_asistencias, promedio_robos, promedio_bloqueos, promedio_rebotes, promedio_faltas).\n",
    "\n",
    "Un analista de la cadena ESPN está trabajando con un RDD que corresponde a la primera ronda de playoffs y que tiene el siguiente formato: (id_jugador, id_partido, timestamp, cantidad_puntos, cantidad_rebotes, cantidad_bloqueos, cantidad_robos, cantidad_asistencias, cantidad_faltas).\n",
    "\n",
    "En base a estos RDDs se quiere programar en PySpark un programa que genere un RDD con los nombres (sin duplicados) de los jugadores que lograron en algún partido de playoffs una cantidad de asistencias mayor a su promedio histórico (el de la temporada regular). Link\n",
    "\n",
    "Llamaremos: rdd_po: RDD con los datos de los playoffs. rdd_tr: RDD con los datos de temporada regular\n",
    "\n",
    "Resolucion: https://github.com/idontdomath/datos-spark-lesson/blob/master/2019-02/001-examenes-2015-2016.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos para simplificar el formato, que puede obtenerse con un map.\n",
    "# (id_jugador, nombre, promedio_asistencias)\n",
    "players_all_time_stats = [\n",
    "    (1, 'Manu Ginobili', 800),\n",
    "    (2, 'Kobe Bryant', 100),\n",
    "    (3, 'Marc Gasol', 25),\n",
    "    (4, 'James Harden', 1000)]\n",
    "\n",
    "# usamos para simplificar el formato, que puede obtenerse con un map.\n",
    "# (id_jugador, id_partido, timestamp, cantidad_asistencias)\n",
    "scores = [\n",
    "  (1, 1, 1, 100),\n",
    "  (1, 1, 3, 100),\n",
    "  (2, 1, 1, 150),\n",
    "  (2, 1, 3, 150),\n",
    "  (3, 2, 2, 50),\n",
    "  (3, 2, 3, 50),      \n",
    "  (1, 2, 1, 150),\n",
    "  (1, 2, 3, 150),\n",
    "]\n",
    "\n",
    "rdd_jugadores = sc.parallelize(players_all_time_stats)\n",
    "rdd_po = sc.parallelize(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2016-02 Parcial\n",
    "En este ejercicio queremos programar un sistema que recomiende textos a usuarios en base a sus gustos sobre ciertos términos (palabras).\n",
    "\n",
    "Se cuenta con un RDD de textos de la forma (docId, texto) donde texto es un string de longitud variable.\n",
    "\n",
    "Además contamos con un RDD que indica qué términos le gustan o no a cada usuario de la forma (userId, término, score) por ejemplo (23, “calesita”, -2).\n",
    "\n",
    "Se pide programar en Spark un programa que calcule el score total de cada documento para cada usuario generando un RDD de la forma (userId, docId, score) en donde el score es simplemente la suma de los scores del usuario para los términos que aparecen en el documento.\n",
    "\n",
    "Puede haber términos en los documentos para los cuales no exista score de algunos usuarios, en estos casos simplemente los consideramos neutros (score=0)\n",
    "\n",
    "https://github.com/idontdomath/datos-spark-lesson/blob/master/2019-02/001-examenes-2015-2016.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    (1, 'pablo honey'),\n",
    "    (2, 'the bends'),\n",
    "    (3, 'ok computer'),\n",
    "    (4, 'kid a'),\n",
    "    (5, 'amnesiac'),\n",
    "    (6, 'hail to the thief'),\n",
    "    (7, 'in rainbows'),\n",
    "    (8, 'the king of limbs'),\n",
    "    (9, 'a moon shaped pool')\n",
    "]\n",
    "\n",
    "# (doc_id, text)\n",
    "\n",
    "scores = [\n",
    "    ('thom', 'pablo', 1),\n",
    "    ('thom', 'honey', 1),\n",
    "    ('martin', 'pablo', -1),\n",
    "    ('martin', 'honey', -1),\n",
    "    ('martin', 'ok', 30),\n",
    "    ('martin', 'computer', 30)\n",
    "]\n",
    "\n",
    "# (used_id, termino, score)\n",
    "\n",
    "documents_rdd = sc.parallelize(documents)\n",
    "scores_rdd = sc.parallelize(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer Cuatrimestre de 2018. Examen parcial, primera oportunidad.\n",
    "Nintendo of America (EEUU) tiene información de ventas de videojuegos físicas mensuales totalizadas en EEUU las cuales se realizan en cadenas de tiendas de videojuegos en el siguiente RDD: (id_videojuego, id_tienda, mes, anio, total_ventas_mensuales).\n",
    "\n",
    "Por otro lado tenemos un RDD con información de las tiendas y de su ubicación (id_tienda, direccion, latitud, longitud, codigo_postal, estado). Con esta información escribir un programa en pySpark para obtener la tienda que realizó menor cantidad de ventas en el estado de \"Georgia\" en todo el año 2017.\n",
    "\n",
    "Criterio: Es importante filtrar los datos que son necesarios antes de comenzar a trabajar, si no lo hacen se descuenta un min de 5ptos. Hay descuentos de 3 ptos si realizan operaciones de mas, o ineficientes (por ejemplo realizar un takeordered cuando necesitan solo obtener mínimo). Si los formatos para realizar el join no se corresponde a (K, V) descuento de 5 puntos.\n",
    "\n",
    "Resolucion: https://github.com/idontdomath/datos-spark-lesson/blob/master/2019-02/003-examenes-2018.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales = [\n",
    "    (1, 1, '01', '2017', 500), # sera la minima\n",
    "    (1, 2, '01', '2017', 500),\n",
    "    (1, 2, '01', '2017', 500),\n",
    "    (1, 2, '01', '2017', 500),\n",
    "    (1, 2, '01', '2017', 500),\n",
    "    (1, 1, '01', '2016', 500),\n",
    "    (1, 2, '01', '2016', 500),\n",
    "    (1, 2, '01', '2016', 500),\n",
    "    (1, 2, '01', '2016', 500),\n",
    "    (1, 2, '01', '2016', 500),\n",
    "    (2, 3, '01', '2017', 500),\n",
    "    (2, 3, '01', '2017', 500),\n",
    "    (2, 3, '01', '2017', 500),\n",
    "    (2, 3, '01', '2017', 500),\n",
    "    (2, 3, '01', '2017', 500),\n",
    "    (4, 3, '01', '2017', 500),\n",
    "    (4, 3, '01', '2017', 500),\n",
    "    (4, 3, '02', '2017', 500),\n",
    "    (4, 3, '03', '2017', 500),    \n",
    "\n",
    "]\n",
    "\n",
    "#  (id_videojuego, id_tienda, mes, anio, total_ventas_mensuales).\n",
    "\n",
    "stores = [\n",
    "    (1 , 'address 1', -1, -1, '30002', 'Georgia'),\n",
    "    (2 , 'address 2', -2, -2, '30003', 'Georgia'),\n",
    "    (3 , 'address 2', -3, -3, '30004', 'Georgia'),\n",
    "    (4 , 'address 2', -4, -4, '10119', 'New York')    \n",
    "]\n",
    "\n",
    "# (id_tienda, direccion, latitud, longitud, codigo_postal, estado)\n",
    "\n",
    "sales_rdd = sc.parallelize(sales)\n",
    "stores_rdd = sc.parallelize(stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer Cuatrimestre de 2018. Examen parcial, tercera oportunidad\n",
    "El GCPD (Gotham City Police Dept) recolecta la información de casos policiales que acontecen en Ciudad Gótica. Esta información se encuentra guardada en un archivo con el siguiente formato: (fecha, id_caso, descripción, estado_caso, categoría, latitud, longitud).\n",
    "\n",
    "Los posibles estados que puede tener un caso son 1: caso abierto, 2: caso resuelto, 3: cerrado sin resolución. Las fechas se encuentran en el formato YYYY-MM-DD.\n",
    "\n",
    "Por otro lado el comisionado Gordon guarda un registro detallado sobre en cuáles casos fue activada la batiseñal para pedir ayuda del vigilante, Batman. Esta información se encuentra en un archivo con el siguiente formato (id_caso, respuesta), siendo campo respuesta si la señal tuvo una respuesta positiva (1) o negativa (0) de parte de él. El sector encargado de las estadísticas oficiales del GCPD quiere analizar las siguientes situaciones:\n",
    "\n",
    "a) Las categorías que hayan incrementado su tasa de resolución en al menos un 10% en el último trimestre, con respecto al trimestre anterior. b) Tasa de participación de Batman por categoría, para los delitos contra la propiedad (que enmarcan las categorías incendio intencional, robo, hurto, y robo de vehículos)\n",
    "\n",
    "Resolución:\n",
    "\n",
    "Primero filter por fecha ult dos trimestres. Luego map con key compuesta categoria y trimestre y value (estado_caso == resuelto, estado == sin_resolucion or estado == resuelto). ReduceByKey, donde sumes los dos values. Un map cambiando la clave a categoria, y dejando en value el trimestre y los dos valores. Un groupByKey para juntar en la misma categoria los dos valores de cada trimestre. Por ultimo un filter que compare las tasas de cada trimestre, para ver cuales cumplen con la condición. Collect al final. Vale 8 puntos.\n",
    "\n",
    "Si no filtran al ppio descuento 3 puntos. Map u operaciones innecesarias descuento de 3 puntos mínimo.\n",
    "\n",
    "Otra solución posible es: se pueden generar dos RDD: uno para cada trimestre. Sobre esos dos RDD, se calculan las tasas de resolución por categoría y luego se los joinea por el campo de categoría para poder verificar la condición de que haya incrementado la resolución en un 10%. Esta segunda alternativa no es escalable, ya que si tuvieramos que trabajar con mas trimestres o hacer el corte por mes o por dia no tendria sentido generar un rdd por dia. (Descuento de 5puntos)\n",
    "\n",
    "b) Será necesario primero filtrar el RDD por las categorías necesarias para reducir el volumen de información (descuento de 3 puntos si lo hacen después) y luego joinear por id_caso con el segundo RDD utilizando un left join (descuento de 3 puntos si se usa inner, 5 puntos si las claves no coinciden, no sirve). El left join nos dará aquellos casos en los que batman fue llamado y en los que no fue llamados. A partir de ahí, podemos mapear algo del estilo (categoria, (fue_llamado, 1)) y con un reduceByKey calcular la tasa de participación. El campo “fue_llamado” será 1 si respondió a la batiseñal o 0 en caso contrario. El b vale 7 puntos.\n",
    "\n",
    "Resolucion: https://github.com/idontdomath/datos-spark-lesson/blob/master/2019-02/003-examenes-2018.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (fecha, id_caso, descripción, estado_caso, categoría, latitud, longitud).\n",
    "cases = [(\"2019-01-01\", 1, \"case 1\", 2, \"otro delito\", -1, -1), \n",
    "         (\"2019-06-01\", 2, \"case 2\", 2, \"robo\", -1, -1),\n",
    "         (\"2019-06-01\", 3, \"case 2\", 3, \"robo\", -1, -1),         \n",
    "         (\"2019-06-01\", 4, \"case 2\", 1, \"robo\", -1, -1),         \n",
    "         (\"2019-06-01\", 5, \"case 2\", 2, \"robo\", -1, -1),\n",
    "         (\"2019-09-01\", 6, \"case 2\", 2, \"robo\", -1, -1),\n",
    "         (\"2019-09-01\", 7, \"case 2\", 2, \"robo\", -1, -1),         \n",
    "         (\"2019-09-01\", 8, \"case 2\", 2, \"robo\", -1, -1),\n",
    "         (\"2019-09-01\", 9, \"case 2\", 2, \"robo\", -1, -1),\n",
    "         (\"2019-09-01\", 10, \"case 2\", 3, \"robo\", -1, -1),\n",
    "         (\"2019-09-01\", 60, \"case 2\", 3, \"robo\", -1, -1),\n",
    "         (\"2019-09-01\", 70, \"case 2\", 3, \"robo\", -1, -1),         \n",
    "         (\"2019-09-01\", 80, \"case 2\", 1, \"robo\", -1, -1),\n",
    "         (\"2019-09-01\", 90, \"case 2\", 2, \"robo\", -1, -1),\n",
    "         (\"2019-09-01\", 100, \"case 2\", 2, \"robo\", -1, -1),\n",
    "         (\"2019-09-01\", 600, \"case 2\", 2, \"robo\", -1, -1),\n",
    "         (\"2019-09-01\", 700, \"case 2\", 3, \"robo\", -1, -1),         \n",
    "         (\"2019-09-01\", 800, \"case 2\", 1, \"robo\", -1, -1),\n",
    "         (\"2019-09-01\", 900, \"case 2\", 1, \"robo\", -1, -1),\n",
    "         (\"2019-09-01\", 1000, \"case 2\", 1, \"robo\", -1, -1),\n",
    "         (\"2019-09-01\", 6000, \"case 2\", 2, \"robo\", -1, -1),\n",
    "         (\"2019-09-01\", 7000, \"case 2\", 2, \"robo\", -1, -1),         \n",
    "         (\"2019-09-01\", 8000, \"case 2\", 3, \"robo\", -1, -1),\n",
    "         (\"2019-09-01\", 9000, \"case 2\", 1, \"robo\", -1, -1),\n",
    "         (\"2019-09-01\", 10000, \"case 2\", 2, \"robo\", -1, -1),\n",
    "         (\"2019-06-01\", 92, \"case 2\", 2, \"hurto\", -1, -1),\n",
    "         (\"2019-06-01\", 93, \"case 2\", 3, \"hurto\", -1, -1),         \n",
    "         (\"2019-06-01\", 94, \"case 2\", 3, \"hurto\", -1, -1),         \n",
    "         (\"2019-06-01\", 95, \"case 2\", 3, \"hurto\", -1, -1),\n",
    "         (\"2019-09-01\", 96, \"case 2\", 2, \"hurto\", -1, -1),\n",
    " \n",
    "        ]\n",
    "\n",
    "# (id_caso, respuesta)\n",
    "batsignal = [(1,1),\n",
    "         (2,1),\n",
    "         (3,0),\n",
    "         (4,0),\n",
    "         (5,1),\n",
    "         (6,0),\n",
    "         (7,1),\n",
    "         (8,0),\n",
    "         (9,1),\n",
    "         (10,0),         \n",
    "         (60,0),\n",
    "         (70,1),\n",
    "         (80,1),\n",
    "         (90,1),\n",
    "         (100,1),\n",
    "         (600,0),\n",
    "         (700,1),\n",
    "         (800,0),\n",
    "         (900,1),\n",
    "         (1000,1),\n",
    "         (6000,0),\n",
    "         (7000,1),\n",
    "         (8000,0),\n",
    "         (9000,1),\n",
    "         (10000,1),\n",
    "         (92,0),\n",
    "         (93,0),             \n",
    "         (94,0),\n",
    "         (95,0),             \n",
    "         (96,1)             \n",
    "        ]\n",
    "\n",
    "cases_rdd = sc.parallelize(cases)\n",
    "batsignal_rdd = sc.parallelize(batsignal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARCIAL\n",
    "A partir de la plataforma online (e-shop) de los países en los que opera, Nintendo tiene información de ventas de videojuegos diarias digitales por país en el siguiente RDD: (id_videojuego, codigo_pais, fecha, visitas_diarias, total_ventas_diarias).\n",
    "\n",
    "Por otro lado se tienen otro RDD que tiene información de todos los videojuegos que se venden en su plataforma con el siguiente formato (id_videojuego, titulo, rating_pegi, rating_esbr). Tener en cuenta que un mismo videojuego se puede vender en distintos países y esos nos permitirá obtener métricas a nivel global.\n",
    "\n",
    "Con esta información escribir un programa en pySpark que permita:\n",
    "\n",
    "a) Obtener el videojuego con más ventas digitales globales (es decir en todos los países) en un RDD con el siguiente formato: (id_videojuego, titulo, total), siendo total la cantidad total de ventas digitales globales\n",
    "\n",
    "b) Para el videojuego con mas ventas, obtener cual es el país para el cual ser registra una mayor tasa de conversión (es decir, mayor total_ventas_diarias / visitas_diarias)\n",
    "\n",
    "Resolucion: https://github.com/CrossNox/7506-OD2/blob/master/spark/2017C2_2_VentasDeJuegos.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_ventas_diarias = [\n",
    "    (1, 'AR', '2018-01-01', 30, 10),\n",
    "    (1, 'ES', '2018-01-01', 23, 13),\n",
    "    (2, 'US', '2018-01-04', 45, 5),\n",
    "    (2, 'MX', '2018-01-04', 20, 10),\n",
    "    (2, 'US', '2018-01-06', 50, 15),\n",
    "    (3, 'AR', '2018-01-06', 10, 2),\n",
    "    (1, 'US', '2018-01-06', 14, 4),\n",
    "    (3, 'ES', '2018-01-10', 34, 11),\n",
    "    (4, 'ES', '2018-01-11', 42, 24),\n",
    "    (4, 'US', '2018-01-11', 83, 34),\n",
    "    (4, 'AR', '2018-01-11', 27, 20),\n",
    "    (4, 'MX', '2018-01-11', 47, 18),\n",
    "    (4, 'AR', '2018-01-20', 10, 0),\n",
    "    (4, 'US', '2018-01-21', 34, 2),\n",
    "    (4, 'ES', '2018-01-21', 25, 7)\n",
    "]\n",
    "\n",
    "datos_videojuegos = [\n",
    "    (1, 'Zelda: Breath of the Wild', 9, 8),\n",
    "    (2, 'Mario Kart', 9, 7),\n",
    "    (3, 'Splatoon 2', 11, 8),\n",
    "    (4, 'Monster Hunter Generations Ultimate', 13, 10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARCIAL\n",
    "Se cuenta con un RDD con información sobre patentamientos de autos con la siguiente información (patente, marca, modelo, versión, tipo_vehiculo, provincia, fecha), donde tipo_vehiculo indica si la unidad patentada es auto, pickup, camión o moto.\n",
    "\n",
    "Se pide generar un programa en pySpark que indique la marca y modelo del auto más patentado por tipo de vehículo en la provincia de Buenos Aires en el mes de Abril de 2017.\n",
    "\n",
    "Resolucion: https://github.com/CrossNox/7506-OD2/blob/master/spark/2017C2_1_Patentamientos.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Creamos algunos datos para poder hacer el seguimiento de la resolución.\n",
    "#  El resultado final debería ser ('Chevrolet', 'Sonic'), ('Ford', 'Cargo 712') y ('Honda', 'Hornet 160R').\n",
    "\n",
    "datos_patentamientos = [\n",
    "    ('MHG 100', 'Fiat', 'Siena', 1, 'auto', 'Buenos Aires', '2017-03-15'),\n",
    "    ('MHG 101', 'Ford', 'Cargo 712', 2, 'camion', 'Chaco', '2017-03-19'),\n",
    "    ('MHG 102', 'Ford', 'Cargo 712', 4, 'camion', 'Buenos Aires', '2017-04-01'),\n",
    "    ('MHG 103', 'Fiat', 'Siena', 2, 'auto', 'Buenos Aires', '2017-04-02'),\n",
    "    ('MHG 104', 'Chevrolet', 'Sonic', 1, 'auto', 'Buenos Aires', '2017-04-02'),\n",
    "    ('MHG 105', 'Fiat', 'Siena', 3, 'auto', 'Uruguay', '2017-04-03'),\n",
    "    ('MHG 106', 'Fiat', 'Siena', 1, 'auto', 'Buenos Aires', '2017-04-05'),\n",
    "    ('MHG 107', 'Chevrolet', 'Sonic', 2, 'auto', 'Buenos Aires', '2017-04-17'),\n",
    "    ('MHG 108', 'Chevrolet', 'Sonic', 1, 'auto', 'Buenos Aires', '2017-04-19'),\n",
    "    ('MHG 109', 'Ford', 'Cargo 712', 4, 'camion', 'Buenos Aires', '2017-04-19'),\n",
    "    ('MHG 110', 'Ford', 'Cargo 712', 2, 'camion', 'Buenos Aires', '2017-04-19'),\n",
    "    ('MHG 111', 'Fiat', 'Siena', 3, 'auto', 'Cordoba', '2017-04-20'),\n",
    "    ('MHG 112', 'Chevrolet', 'Sonic', 2, 'auto', 'Buenos Aires', '2017-04-21'),\n",
    "    ('MHG 113', 'Fiat', 'Sedan', 2, 'auto', 'Buenos Aires', '2017-04-23'),\n",
    "    ('MHG 114', 'Fiat', 'Sedan', 1, 'auto', 'Buenos Aires', '2017-04-24'),\n",
    "    ('MHG 115', 'Honda', 'Hornet 160R', 1, 'moto', 'Buenos Aires', '2017-04-25'),\n",
    "    ('MHG 116', 'Honda', 'Hornet 160R', 1, 'moto', 'Buenos Aires', '2017-04-25'),\n",
    "    ('MHG 117', 'Ducati', 'SuperSport', 1, 'moto', 'Buenos Aires', '2017-04-26'),\n",
    "    ('MHG 118', 'Scania', '420', 4, 'camion', 'Buenos Aires', '2017-04-26')\n",
    "]\n",
    "\n",
    "rdd_pt = sc.parallelize(datos_patentamientos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parcial\n",
    "Una red social almacena el contenido de los chats entre sus usuarios en un RDD que tiene registros con el siguiente formato: (chat_id, user_id, nickname, text). Queremos saber cuál es el usuario (user_id) que mas preguntas hace en sus chats, contabilizamos una pregunta por cada caracter “?” que aparezca en el campo text. Programar en Spark un programa que identifique al usuario preguntón.\n",
    "\n",
    "Resolucion: https://colab.research.google.com/drive/1lHA82VFp-yr9sH5ttxOoyzOV--zKhD7y#scrollTo=cicGxWn2xPsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "chats = [\n",
    "    (1, 1, 'damu', 'Qué es esto?'),\n",
    "    (2, 2, 'martin', 'Un chat!???'),\n",
    "    (3, 1, 'damu', 'Ahhh! Y de donde salio? Whatsapp?'),\n",
    "    (4, 2, 'martin', 'Sí! Cómo sabias????'),\n",
    "    (5, 1, 'damu', 'Adivine'),\n",
    "    (6, 3, 'luis', 'Hola!')\n",
    "]\n",
    "\n",
    "chats_rdd = sc.parallelize(chats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parcial\n",
    "\n",
    "UBER almacena en un cluster todos los datos sobre el movimiento y viajes de todos sus vehículos. Existe un proceso que nos devuelve un RDD llamado trip_summary con los siguientes campos: (driver_id, car_id, trip_id, customer_id, date (YYYYMMDD), distance_traveled), Programar usando Py Spark un programa que nos indique cual fue el conductor con mayor promedio de distancia recorrida por viaje para Abril de 2016.\n",
    "\n",
    "Resolucion: https://colab.research.google.com/drive/1D_I6NcHvpIz3r25MHMY25j1n9Zulba3S#scrollTo=cicGxWn2xPsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = [\n",
    "    (1, 1, 1, 1, '20160101', 10),\n",
    "    (2, 2, 2, 2, '20160202', 20),\n",
    "    (1, 1, 3, 1, '20160402', 15),\n",
    "    (1, 1, 4, 3, '20160405', 20),\n",
    "    (2, 2, 5, 4, '20160410', 25),\n",
    "    (3, 3, 6, 3, '20160415', 15),\n",
    "    (2, 2, 7, 1, '20160420', 40),\n",
    "    (3, 3, 8, 2, '20160505', 80)\n",
    "]\n",
    "\n",
    "trips_rdd = sc.parallelize(trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parcial\n",
    "1) El servicio meteorológico registra datos del tiempo para todas las ciudades donde posee una base de medición.\n",
    "Esta información se encuentra en dos RDD. \n",
    "\n",
    "En el primero se tiene información de las bases de medición: (ID_BASE, NOMBRE, PCIA, CIUDAD, LAT, LON).\n",
    "\n",
    "El segundo posee información sobre las mediciones en sí: (TIMESTAMP, ID_BASE, TEMPERATURA, HUMEDAD, PRESIÓN, DIRECCIÓN VIENTO, VELOCIDAD VIENTO).\n",
    "\n",
    "Se desea obtener un reporte para las bases de la Provincia de Buenos Aires. \n",
    "\n",
    "El mismo debe informar los ID y nombre de bases de medición que hayan registrado una variación de temperatura promedio mensual mayor al 30% en el año 2018 (dada la temperatura promedio de un mes, esta se debe comparar con el promedio del mes anterior, para determinar la variación porcentual).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Spark Session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# create the Spark Context\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#Datos aleatorios extraidos de la web para aportar un set ejemplo y utilización de random\n",
    "\n",
    "#CANTIDAD de datos\n",
    "m = 1000\n",
    "\n",
    "provincias_a = ['Álava', 'Albacete','Alicante','Almería', 'Asturias','Ávila','Badajoz','Barcelona','Cordoba','Cordoba','Cordoba','Cordoba','Cordoba','Cordoba','Cordoba','Cordoba','Cordoba','Cordoba','Cordoba','Cordoba','Cordoba']\n",
    "ciudades_a = [\"Andalucía\", \"Aragón\", \"Canarias\", \"Cantabria\", \"Castilla y León\", \"Castilla-La Mancha\", \"Cataluña\", \"Ceuta\", \"Comunidad Valenciana\", \"Comunidad de Madrid\", \"Extremadura\", \"Galicia\", \"Islas Baleares\", \"La Rioja\", \"Melilla\", \"Navarra\", \"País Vasco\", \"Principado de Asturias\", \"Región de Murcia\"]\n",
    "nombres_bases = [\"one\", \"two\", \"three\", \"four\", \"five\",\"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"eleven\"]\n",
    "direcciones_a = [\"S\",\"E\",\"O\",\"N\",\"SO\",\"SE\",\"NE\",\"NO\",\"h\",\"m\",\"y\",\"x\"]\n",
    "\n",
    "nombres = []\n",
    "provincias = []\n",
    "ciudades = []\n",
    "bases_id = []\n",
    "direcciones = []\n",
    "\n",
    "for i in range(m):\n",
    "    j = random.randint(0,9) #para que se correspondan las mismas id\n",
    "    bases_id.append(j)\n",
    "    nombres.append(nombres_bases[j-1])\n",
    "    ciudades.append(choice(ciudades_a))\n",
    "    provincias.append(choice(provincias_a))\n",
    "    direcciones.append(choice(direcciones_a))\n",
    "\n",
    "#CREACION DE FECHAS===============================================================================================\n",
    "\n",
    "def str_time_prop(start, end, format, prop):\n",
    "    stime = time.mktime(time.strptime(start, format))\n",
    "    etime = time.mktime(time.strptime(end, format))\n",
    "    ptime = stime + prop * (etime - stime)\n",
    "    return time.strftime(format, time.localtime(ptime))\n",
    "\n",
    "def random_date(start, end, prop):\n",
    "    return str_time_prop(start, end, '%m-%d-%Y', prop)\n",
    "\n",
    "dates = []\n",
    "\n",
    "for i in range(m):\n",
    "    dates.append( datetime.strptime(random_date(\"1-1-2016\", \"1-1-2020\", random.random()), '%m-%d-%Y'))\n",
    "\n",
    "#CREACION DEL SET DE DATOS===================================================================================\n",
    "\n",
    "df = pd.DataFrame({'ID_BASE': bases_id,\n",
    "                   'NOMBRE': nombres,\n",
    "                   'PCIA': provincias,\n",
    "                   'CIUDAD':ciudades,}\n",
    "                  )\n",
    "\n",
    "df2 = pd.DataFrame({'TIMESTAMP': sorted(dates),\n",
    "                   'ID_BASE': bases_id,\n",
    "                   'TEMPERATURA': random.sample(range(0,m),k=m),\n",
    "                   'HUMEDAD':random.sample(range(0,m),k=m),\n",
    "                   'PRESION':random.sample(range(0,m),k=m),\n",
    "                   'DIRECCION VIENTO':direcciones,\n",
    "                   'VELOCIDAD VIENTO':random.sample(range(0,m),k=m),}\n",
    "                  )\n",
    "\n",
    "#MODIFICACION DE LOS DATOS DE LA COLUMNDA  TIMESTAP A DATE CORRECTAMENTE\n",
    "df2[\"TIMESTAMP\"] = pd.to_datetime(df2[\"TIMESTAMP\"])\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "df1 = sqlContext.createDataFrame(df)\n",
    "df2 = sqlContext.createDataFrame(df2)\n",
    "bases = df1.rdd\n",
    "mediciones = df2.rdd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parcial\n",
    "1) Dado los acontecimientos en USA, deseamos obtener datos que nos den mayor información sobre las muertes de personas de raza negra por parte de oficiales de policía.\n",
    "\n",
    "Para ello, tenemos un csv con información sobre las muertes por parte de oficiales de policía en USA desde 2015 hasta 2017: (name, date, race, city, state)\n",
    "\n",
    "Y otro csv con información sobre el porcentaje de cada raza en las ciudades de USA: (state, city, share_white, share_black, share_native_american, share_asian, share_hispanic) Se pide:\n",
    "\n",
    "a) Obtener el estado con mayor porcentaje de muertes de personas de raza negra teniendo en cuenta la cantidad total de muertes por parte de oficiales en ese estado. (10 pts)\n",
    "\n",
    "b) Obtener los 10 estados con mayor diferencia entre el porcentaje de muertes y el porcentaje de gente de raza negra en ese estado. Para ello, considerar el porcentaje de raza de un estado como el promedio de los valores de sus ciudades. Por ejemplo si en Texas el porcentaje de muertes de personas de raza negra por parte de la policía es del 36% y el promedio de share_black para Texas es 24% la diferencia es 0.12. (15 pts) Resolver ambos puntos usando la API de RDDs de PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "muertes = [\n",
    "    ('Juan', '2016-01-01', 'negra','ciudad1','estado1'),\n",
    "    ('Gerardo', '2016-01-01', 'negra','ciudad1','estado1'),\n",
    "    ('Silvia', '2016-01-01', 'blanca','ciudad2','estado2'),\n",
    "    ('Margarita', '2016-01-01', 'blanca','ciudad3','estado2'),\n",
    "    ('Andres', '2016-01-01', 'asiatico','ciudad4','estado3'),\n",
    "    ('Pablo', '2016-01-01', 'asiatico','ciudad5','estado3')\n",
    "]\n",
    "\n",
    "ciudades = [\n",
    "    ('estado1', 'ciudad1', 10,40,20,20,10),\n",
    "    ('estado2', 'ciudad2', 40,10,20,20,10),\n",
    "    ('estado2', 'ciudad3', 10,10,20,20,40),\n",
    "    ('estado3', 'ciudad4', 15,15,15,15,40),\n",
    "    ('estado3', 'ciudad5', 70,20,0,10,0)\n",
    "]\n",
    "\n",
    "muertes_rdd = sc.parallelize(muertes)\n",
    "ciudades_rdd = sc.parallelize(ciudades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Juan', '2016-01-01', 'negra', 'ciudad1', 'estado1'),\n",
       " ('Gerardo', '2016-01-01', 'negra', 'ciudad1', 'estado1'),\n",
       " ('Silvia', '2016-01-01', 'blanca', 'ciudad2', 'estado2'),\n",
       " ('Margarita', '2016-01-01', 'blanca', 'ciudad3', 'estado2'),\n",
       " ('Andres', '2016-01-01', 'asiatico', 'ciudad4', 'estado3'),\n",
       " ('Pablo', '2016-01-01', 'asiatico', 'ciudad5', 'estado3')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muertes_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('estado1', 'ciudad1', 10, 40, 20, 20, 10),\n",
       " ('estado2', 'ciudad2', 40, 10, 20, 20, 10),\n",
       " ('estado2', 'ciudad3', 10, 10, 20, 20, 40),\n",
       " ('estado3', 'ciudad4', 15, 15, 15, 15, 40),\n",
       " ('estado3', 'ciudad5', 70, 20, 0, 10, 0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ciudades_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Punto A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "muertes_por_estado = muertes_rdd.map(lambda x: (x[4], (1 if x[2]=='negra' else 0, 1)))\\\n",
    "    .reduceByKey(lambda x, y: (x[0]+y[0],x[1]+y[1]))\\\n",
    "    .map(lambda x: (x[0], float(x[1][0]/x[1][1])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('estado1', 100.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muertes_por_estado.reduce(lambda x, y: x if x[1]>y[1] else y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Punto B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "negros_por_ciudad = ciudades_rdd.map(lambda x: (x[0], (x[3], 1)))\\\n",
    "        .reduceByKey(lambda x, y: (x[0]+y[0],x[1]+y[1]))\\\n",
    "        .map(lambda x: (x[0], float(x[1][0]/x[1][1]))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('estado1', 60.0), ('estado2', -10.0), ('estado3', -17.5)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agrupo = muertes_por_estado.join(negros_por_ciudad).map(lambda x: (x[0],(x[1][0]-x[1][1]))) #Falta el abs\n",
    "agrupo.takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
